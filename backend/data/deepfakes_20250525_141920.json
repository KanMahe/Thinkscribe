[
    {
        "title": "Detecting Deepfakes with Self-Blended Images",
        "authors": [
            "Kaede Shiohara",
            "T. Yamasaki"
        ],
        "year": 2022,
        "abstract": "In this paper, we present novel synthetic training data called self-blended images (SBIs) to detect deepfakes. SBIs are generated by blending pseudo source and target images from single pristine images, reproducing common forgery artifacts (e.g., blending boundaries and statistical inconsistencies between source and target images). The key idea behind SBIs is that more general and hardly recognizable fake samples encourage classifiers to learn generic and robust representations without overfitting to manipulation-specific artifacts. We compare our approach with state-of-the-art methods on FF++, CDF, DFD, DFDC, DFDCP, and FFIW datasets by following the standard cross-dataset and cross-manipulation protocols. Extensive experiments show that our method improves the model generalization to unknown manipulations and scenes. In particular, on DFDC and DFDCP where existing methods suffer from the domain gap between the training and test sets, our approach outperforms the baseline by 4.90% and 11.78% points in the cross-dataset evaluation, respectively. Code is available at https://github.com/mapooon/SelfBlendedImages.",
        "url": "https://www.semanticscholar.org/paper/ef3b913e6509077c67e678674e2ba33d99a201a5"
    },
    {
        "title": "Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics",
        "authors": [
            "Shan Jia",
            "Reilin Lyu",
            "Kangran Zhao",
            "Yize Chen",
            "Zhiyuan Yan",
            "Yan Ju",
            "Chuanbo Hu",
            "Xin Li",
            "Baoyuan Wu",
            "Siwei Lyu"
        ],
        "year": 2024,
        "abstract": "DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.",
        "url": "https://www.semanticscholar.org/paper/cbb90a94d0cb3334e32c4ae910ac8a3e03dd18da"
    },
    {
        "title": "ASVspoof 5: Crowdsourced Speech Data, Deepfakes, and Adversarial Attacks at Scale",
        "authors": [
            "Xin Wang",
            "H\u00e9ctor Delgado",
            "Hemlata Tak",
            "Jee-weon Jung",
            "Hye-jin Shim",
            "M. Todisco",
            "Ivan Kukanov",
            "Xuechen Liu",
            "Md. Sahidullah",
            "Tomi Kinnunen",
            "Nicholas Evans",
            "K. Lee",
            "Junichi Yamagishi"
        ],
        "year": 2024,
        "abstract": "ASVspoof 5 is the fifth edition in a series of challenges that promote the study of speech spoofing and deepfake attacks, and the design of detection solutions. Compared to previous challenges, the ASVspoof 5 database is built from crowdsourced data collected from a vastly greater number of speakers in diverse acoustic conditions. Attacks, also crowdsourced, are generated and tested using surrogate detection models, while adversarial attacks are incorporated for the first time. New metrics support the evaluation of spoofing-robust automatic speaker verification (SASV) as well as stand-alone detection solutions, i.e., countermeasures without ASV. We describe the two challenge tracks, the new database, the evaluation metrics, baselines, and the evaluation platform, and present a summary of the results. Attacks significantly compromise the baseline systems, while submissions bring substantial improvements.",
        "url": "https://www.semanticscholar.org/paper/9c94cce341eb73f0e2263fa477d619bfe1164ecb"
    },
    {
        "title": "Deepfakes: current and future trends",
        "authors": [
            "\u00c1. F. Gamb\u00edn",
            "Anis Yazidi",
            "Athanasios Vasilakos",
            "H. Haugerud",
            "Y. Djenouri"
        ],
        "year": 2024,
        "abstract": "Advances in Deep Learning (DL), Big Data and image processing have facilitated online disinformation spreading through Deepfakes. This entails severe threats including public opinion manipulation, geopolitical tensions, chaos in financial markets, scams, defamation and identity theft among others. Therefore, it is imperative to develop techniques to prevent, detect, and stop the spreading of deepfake content. Along these lines, the goal of this paper is to present a big picture perspective of the deepfake paradigm, by reviewing current and future trends. First, a compact summary of DL techniques used for deepfakes is presented. Then, a review of the fight between generation and detection techniques is elaborated. Moreover, we delve into the potential that new technologies, such as distributed ledgers and blockchain, can offer with regard to cybersecurity and the fight against digital deception. Two scenarios of application, including online social networks engineering attacks and Internet of Things, are reviewed where main insights and open challenges are tackled. Finally, future trends and research lines are discussed, pointing out potential key agents and technologies.",
        "url": "https://www.semanticscholar.org/paper/f8c523a69250296becba5ce0850292ea114bd36c"
    },
    {
        "title": "DF-RAP: A Robust Adversarial Perturbation for Defending Against Deepfakes in Real-World Social Network Scenarios",
        "authors": [
            "Zuomin Qu",
            "Zuping Xi",
            "Wei Lu",
            "Xiangyang Luo",
            "Qian Wang",
            "B. Li"
        ],
        "year": 2024,
        "abstract": "The misuse of Deepfakes to create unauthorized fake facial images and videos poses a growing threat to personal privacy and social stability. Proactive defense algorithms have been proposed to prevent this fraud by injecting adversarial perturbations into facial images. However, these perturbations are sensitive to the lossy compression on online social networks (OSNs). Recent studies have attempted to produce compression resistance by modeling compression at the pixel level. However, accurate modeling is challenging due to the customization of proprietary compression mechanisms by different OSNs. In this paper, we propose a Robust Adversarial Perturbation (DF-RAP) that provides persistent protection for facial images under OSN compression. Specifically, a novel Compression Approximation GAN (ComGAN) is designed to explicitly model OSN compression. The well-trained ComGAN is then incorporated as a sub-module of the target Deepfake model to derive DF-RAP. Furthermore, we reveal a commonality among various OSNs, i.e., that the lossy compression employed tends to destroy perturbations. Based on this, a novel objective-level destruction-aware constraint (DAC) is introduced during ComGAN training. The extensive experimental results show that DF-RAP can effectively protect facial images from Deepfakes under complex OSN compression, especially for OSNs employing more stringent compression. We also investigate the lossy operation mechanisms employed by widely used OSN platforms and build an OSN-transmission dataset based on the CelebA to facilitate future research.",
        "url": "https://www.semanticscholar.org/paper/8e8962c96096c66bc8e0e68ca5a76189a40de95c"
    },
    {
        "title": "Generative AI and deepfakes: a human rights approach to tackling harmful content",
        "authors": [
            "Felipe Romero Moreno"
        ],
        "year": 2024,
        "abstract": "ABSTRACT The EU's Artificial Intelligence Act (AIA) introduces necessary deepfake regulations. However, these could infringe on the rights of AI providers and deployers or users, potentially conflicting with privacy and free expression under Articles 8 and 10 of the European Convention on Human Rights, and the General Data Protection Regulation (EU) 2016/679 (GDPR). This paper critically examines how an unmodified AIA could enable voter manipulation, blackmail, and the generation of sexual abusive content, facilitating misinformation and potentially harming millions, both emotionally and financially. Through analysis of the AIA's provisions, GDPR's regulations, relevant case law, and academic literature, the paper identifies risks for both AI providers and users. While the AIA's yearly review cycle is important, the immediacy of these threats demands swifter action. This paper proposes two key amendments: 1) mandate structured synthetic data for deepfake detection, and 2) classify AI intended for malicious deepfakes as \u2018high-risk\u2019. These amendments, alongside clear definitions and robust safeguards would ensure effective deepfake regulation while protecting fundamental rights. The paper urges policymakers to adopt these amendments during the next review cycle to protect democracy, individual safety, and children. Only then will the AIA fully achieve its aims while safeguarding the freedoms it seeks to uphold.",
        "url": "https://www.semanticscholar.org/paper/df409cc1387abe0ffa7e65c6b90e283b1e37dd6f"
    },
    {
        "title": "Navigating the Dual Nature of Deepfakes: Ethical, Legal, and Technological Perspectives on Generative Artificial Intelligence AI) Technology",
        "authors": [],
        "year": 2024,
        "abstract": "The rapid development of deepfake technology has opened up a range of groundbreaking opportunities while also introducing significant ethical challenges. This paper explores the complex impacts of deepfakes by drawing from fields such as computer science, ethics, media studies, and law. Through a multidisciplinary approach, we examine the technological foundations, uses, and societal effects of deepfakes. Our analysis includes case studies, expert interviews, and a thorough review of existing literature to highlight the dual nature of deepfakes\u2014showcasing their potential benefits in entertainment and education, while also addressing the risks of misinformation and privacy violations. This study emphasizes the urgent need for improved detection methods, ethical guidelines, and strong legal frameworks to address the issues created by deepfakes. It calls for enhanced digital literacy and global cooperation to ensure that the advantages of generative AI are harnessed responsibly, while its inherent risks are minimized. The findings underscore the importance of effective detection strategies, ethical considerations, and legislative reforms to ensure deepfake technology is used in ways that benefit society.",
        "url": "https://www.semanticscholar.org/paper/6242be8ef591473fd1e66283f9978688b72bb324"
    },
    {
        "title": "Two-branch Recurrent Network for Isolating Deepfakes in Videos",
        "authors": [
            "I. Masi",
            "Aditya Killekar",
            "R. Mascarenhas",
            "Shenoy Pratik Gurudatt",
            "Wael AbdAlmageed"
        ],
        "year": 2020,
        "abstract": "The current spike of hyper-realistic faces artificially generated using deepfakes calls for media forensics solutions that are tailored to video streams and work reliably with a low false alarm rate at the video level. We present a method for deepfake detection based on a two-branch network structure that isolates digitally manipulated faces by learning to amplify artifacts while suppressing the high-level face content. Unlike current methods that extract spatial frequencies as a preprocessing step, we propose a two-branch structure: one branch propagates the original information, while the other branch suppresses the face content yet amplifies multi-band frequencies using a Laplacian of Gaussian (LoG) as a bottleneck layer. To better isolate manipulated faces, we derive a novel cost function that, unlike regular classification, compresses the variability of natural faces and pushes away the unrealistic facial samples in the feature space. Our two novel components show promising results on the FaceForensics++, Celeb-DF, and Facebook's DFDC preview benchmarks, when compared to prior work. We then offer a full, detailed ablation study of our network architecture and cost function. Finally, although the bar is still high to get very remarkable figures at a very low false alarm rate, our study shows that we can achieve good video-level performance when cross-testing in terms of video-level AUC.",
        "url": "https://www.semanticscholar.org/paper/004c7dbd5578865ac72cfa7b6ebc51c7fa7cda31"
    },
    {
        "title": "Unmasking deepfakes: A systematic review of deepfake detection and generation techniques using artificial intelligence",
        "authors": [
            "Fakhar Abbas",
            "Araz Taeihagh"
        ],
        "year": 2024,
        "abstract": null,
        "url": "https://www.semanticscholar.org/paper/ea780a33772ea644e35924fa9fd9341a0b45087d"
    }
]