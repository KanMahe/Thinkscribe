[
    {
        "title": "A Revised Generative Evaluation of Visual Dialogue",
        "authors": [
            "Daniela Massiceti",
            "Viveka Kulharia",
            "Puneet K. Dokania",
            "N. Siddharth",
            "Philip H. S. Torr"
        ],
        "year": 2020,
        "abstract": "Evaluating Visual Dialogue, the task of answering a sequence of questions\nrelating to a visual input, remains an open research challenge. The current\nevaluation scheme of the VisDial dataset computes the ranks of ground-truth\nanswers in predefined candidate sets, which Massiceti et al. (2018) show can be\nsusceptible to the exploitation of dataset biases. This scheme also does little\nto account for the different ways of expressing the same answer--an aspect of\nlanguage that has been well studied in NLP. We propose a revised evaluation\nscheme for the VisDial dataset leveraging metrics from the NLP literature to\nmeasure consensus between answers generated by the model and a set of relevant\nanswers. We construct these relevant answer sets using a simple and effective\nsemi-supervised method based on correlation, which allows us to automatically\nextend and scale sparse relevance annotations from humans to the entire\ndataset. We release these sets and code for the revised evaluation scheme as\nDenseVisDial, and intend them to be an improvement to the dataset in the face\nof its existing constraints and design choices.",
        "techniques": [],
        "url": "http://arxiv.org/abs/2004.09272v2"
    },
    {
        "title": "Can AI Assistance Aid in the Grading of Handwritten Answer Sheets?",
        "authors": [
            "Pritam Sil",
            "Parag Chaudhuri",
            "Bhaskaran Raman"
        ],
        "year": 2024,
        "abstract": "With recent advancements in artificial intelligence (AI), there has been\ngrowing interest in using state of the art (SOTA) AI solutions to provide\nassistance in grading handwritten answer sheets. While a few commercial\nproducts exist, the question of whether AI-assistance can actually reduce\ngrading effort and time has not yet been carefully considered in published\nliterature. This work introduces an AI-assisted grading pipeline. The pipeline\nfirst uses text detection to automatically detect question regions present in a\nquestion paper PDF. Next, it uses SOTA text detection methods to highlight\nimportant keywords present in the handwritten answer regions of scanned answer\nsheets to assist in the grading process. We then evaluate a prototype\nimplementation of the AI-assisted grading pipeline deployed on an existing\ne-learning management platform. The evaluation involves a total of 5 different\nreal-life examinations across 4 different courses at a reputed institute; it\nconsists of a total of 42 questions, 17 graders, and 468 submissions. We log\nand analyze the grading time for each handwritten answer while using AI\nassistance and without it. Our evaluations have shown that, on average, the\ngraders take 31% less time while grading a single response and 33% less grading\ntime while grading a single answer sheet using AI assistance.",
        "techniques": [],
        "url": "http://arxiv.org/abs/2408.12870v1"
    },
    {
        "title": "LINKAGE: Listwise Ranking among Varied-Quality References for\n  Non-Factoid QA Evaluation via LLMs",
        "authors": [
            "Sihui Yang",
            "Keping Bi",
            "Wanqing Cui",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "year": 2024,
        "abstract": "Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to\ndiverse potential answers and no objective criterion. The commonly used\nautomatic evaluation metrics like ROUGE or BERTScore cannot accurately measure\nsemantic similarities or answers from different perspectives. Recently, Large\nLanguage Models (LLMs) have been resorted to for NFQA evaluation due to their\ncompelling performance on various NLP tasks. Common approaches include\npointwise scoring of each candidate answer and pairwise comparisons between\nanswers. Inspired by the evolution from pointwise to pairwise to listwise in\nlearning-to-rank methods, we propose a novel listwise NFQA evaluation approach,\nthat utilizes LLMs to rank candidate answers in a list of reference answers\nsorted by descending quality. Moreover, for NF questions that do not have\nmulti-grade or any golden answers, we leverage LLMs to generate the reference\nanswer list of various quality to facilitate the listwise evaluation. Extensive\nexperimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and\nWebGLM show that our method has significantly higher correlations with human\nannotations compared to automatic scores and common pointwise and pairwise\napproaches.",
        "techniques": [],
        "url": "http://arxiv.org/abs/2409.14744v2"
    },
    {
        "title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval\n  Augmented Question Answering",
        "authors": [
            "Rujun Han",
            "Yuhao Zhang",
            "Peng Qi",
            "Yumo Xu",
            "Jenyuan Wang",
            "Lan Liu",
            "William Yang Wang",
            "Bonan Min",
            "Vittorio Castelli"
        ],
        "year": 2024,
        "abstract": "Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research.",
        "techniques": [],
        "url": "http://arxiv.org/abs/2407.13998v2"
    },
    {
        "title": "Text similarity analysis for evaluation of descriptive answers",
        "authors": [
            "Vedant Bahel",
            "Achamma Thomas"
        ],
        "year": 2021,
        "abstract": "Keeping in mind the necessity of intelligent system in educational sector,\nthis paper proposes a text analysis based automated approach for automatic\nevaluation of the descriptive answers in an examination. In particular, the\nresearch focuses on the use of intelligent concepts of Natural Language\nProcessing and Data Mining for computer aided examination evaluation system.\nThe paper present an architecture for fair evaluation of answer sheet. In this\narchitecture, the examiner creates a sample answer sheet for given sets of\nquestion. By using the concept of text summarization, text semantics and\nkeywords summarization, the final score for each answer is calculated. The text\nsimilarity model is based on Siamese Manhattan LSTM (MaLSTM). The results of\nthis research were compared to manually graded assignments and other existing\nsystem. This approach was found to be very efficient in order to be implemented\nin an institution or in an university.",
        "techniques": [],
        "url": "http://arxiv.org/abs/2105.02935v1"
    }
]